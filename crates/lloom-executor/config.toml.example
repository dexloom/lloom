# Example configuration file for Lloom Executor
# Copy this to config.toml and modify as needed

# LLM backend configurations
[[llm_backends]]
name = "openai"
endpoint = "https://api.openai.com/v1"
# API key can be set here or via OPENAI_API_KEY environment variable
# api_key = "your-openai-api-key-here"
supported_models = ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"]
rate_limit = 60  # requests per minute

# Example additional backend (commented out)
# [[llm_backends]]
# name = "anthropic"
# endpoint = "https://api.anthropic.com"
# api_key = "your-anthropic-api-key-here"
# supported_models = ["claude-3-sonnet", "claude-3-opus"]
# rate_limit = 30

# LMStudio local backend configuration
# To use LMStudio:
# 1. Start LMStudio and load a model
# 2. Enable the local server in LMStudio (default: localhost:1234)
# 3. Uncomment and customize the configuration below
# [[llm_backends]]
# name = "lmstudio"
# endpoint = "http://localhost:1234/api/v0"  # LMStudio's enhanced API endpoint
# # No API key needed for local LMStudio
# supported_models = []  # Leave empty for auto-discovery, or specify loaded models
# rate_limit = 100  # Higher rate limit for local processing
#
# Benefits of LMStudio backend:
# - Automatic model discovery from running LMStudio instance
# - Enhanced performance metrics (tokens/sec, time to first token)
# - Model architecture and size information
# - No API costs for local inference

[blockchain]
rpc_url = "https://rpc.sepolia.org"
# contract_address = "0x..."  # Set via CLI argument or environment variable
gas_price_multiplier = 1.2
batch_interval_secs = 300  # 5 minutes
max_batch_size = 100

[network]
port = 9001
# external_address = "/ip4/your.public.ip/tcp/9001"  # Set if behind NAT
bootstrap_nodes = []  # Add known validator nodes here
announce_interval_secs = 300  # 5 minutes