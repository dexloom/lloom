# Example configuration file for Lloom Executor
# Copy this to config.toml and modify as needed

# LLM backend configurations
[[llm_backends]]
name = "openai"
endpoint = "https://api.openai.com/v1"
# API key can be set here or via OPENAI_API_KEY environment variable
# api_key = "your-openai-api-key-here"
supported_models = ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"]
rate_limit = 60  # requests per minute

# Example additional backend (commented out)
# [[llm_backends]]
# name = "anthropic"
# endpoint = "https://api.anthropic.com"
# api_key = "your-anthropic-api-key-here"
# supported_models = ["claude-3-sonnet", "claude-3-opus"]
# rate_limit = 30

[blockchain]
rpc_url = "https://rpc.sepolia.org"
# contract_address = "0x..."  # Set via CLI argument or environment variable
gas_price_multiplier = 1.2
batch_interval_secs = 300  # 5 minutes
max_batch_size = 100

[network]
port = 9001
# external_address = "/ip4/your.public.ip/tcp/9001"  # Set if behind NAT
bootstrap_nodes = []  # Add known accountant nodes here
announce_interval_secs = 300  # 5 minutes